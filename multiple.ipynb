{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration B - Tasks come from different datasets for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.apple.com/simple, https://pypi.apple.com/simple\n",
      "Requirement already satisfied: medmnist in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (1.26.2)\n",
      "Requirement already satisfied: torch in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (2.1.1)\n",
      "Requirement already satisfied: scikit-image in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (0.22.0)\n",
      "Requirement already satisfied: pandas in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (2.1.3)\n",
      "Requirement already satisfied: fire in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (0.5.0)\n",
      "Requirement already satisfied: torchvision in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (0.16.1)\n",
      "Requirement already satisfied: Pillow in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (10.1.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from medmnist) (4.66.1)\n",
      "Requirement already satisfied: six in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from fire->medmnist) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from fire->medmnist) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from pandas->medmnist) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from pandas->medmnist) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from pandas->medmnist) (2023.3)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-image->medmnist) (2023.9.26)\n",
      "Requirement already satisfied: imageio>=2.27 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-image->medmnist) (2.33.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-image->medmnist) (3.2.1)\n",
      "Requirement already satisfied: packaging>=21 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-image->medmnist) (23.2)\n",
      "Requirement already satisfied: scipy>=1.8 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-image->medmnist) (1.11.4)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-image->medmnist) (0.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-learn->medmnist) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from scikit-learn->medmnist) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from torch->medmnist) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from torch->medmnist) (2023.10.0)\n",
      "Requirement already satisfied: filelock in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from torch->medmnist) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from torch->medmnist) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from torch->medmnist) (1.12)\n",
      "Requirement already satisfied: requests in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from torchvision->medmnist) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from jinja2->torch->medmnist) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests->torchvision->medmnist) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests->torchvision->medmnist) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests->torchvision->medmnist) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests->torchvision->medmnist) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from sympy->torch->medmnist) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/angel/.virtualenvs/cs330_project_hw2/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.apple.com/simple, https://pypi.apple.com/simple\n",
      "Requirement already satisfied: tensorboard in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (2.15.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (3.5.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (2.23.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (1.26.2)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (58.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (1.59.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (6.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/angel/.virtualenvs/cs330_project_hw2/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install medmnist\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.multiprocessing\n",
    "import torch.nn.functional as F  # pylint: disable=unused-import\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "from torch import autograd\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import os\n",
    "import math\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMNIST v2.2.3 @ https://github.com/MedMNIST/MedMNIST/\n",
      "Python version: 3.9.13 (v3.9.13:6de2ca5339, May 17 2022, 11:37:23) \n",
      "[Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Looking in indexes: https://pypi.apple.com/simple, https://pypi.apple.com/simple\n",
      "Requirement already satisfied: multiprocess in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (0.70.15)\n",
      "Requirement already satisfied: dill>=0.3.7 in /Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages (from multiprocess) (0.3.7)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/angel/.virtualenvs/cs330_project_hw2/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "platform = sys.platform\n",
    "\n",
    "if platform == \"linux\":\n",
    "    sys.stdout = open('/dev/stdout', 'w')\n",
    "else:\n",
    "    !pip install multiprocess\n",
    "    import multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'medmnist' from '/Users/angel/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages/medmnist/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We first work on a 2D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we read the MedMNIST data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/angel/.medmnist/dermamnist.npz\n",
      "Using downloaded and verified file: /Users/angel/.medmnist/dermamnist.npz\n",
      "Dataset DermaMNIST (dermamnist)\n",
      "    Number of datapoints: 7007\n",
      "    Root location: /Users/angel/.medmnist\n",
      "    Split: train\n",
      "    Task: multi-class\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
      "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
      "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
      "    License: CC BY-NC 4.0\n",
      "Using downloaded and verified file: /Users/angel/.medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: /Users/angel/.medmnist/pathmnist.npz\n",
      "Dataset PathMNIST (pathmnist)\n",
      "    Number of datapoints: 89996\n",
      "    Root location: /Users/angel/.medmnist\n",
      "    Split: train\n",
      "    Task: multi-class\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
      "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
      "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
      "    License: CC BY 4.0\n"
     ]
    }
   ],
   "source": [
    "data_flags = ['dermamnist', 'pathmnist']\n",
    "\n",
    "# preprocessingt\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "download = True\n",
    "all_training_dataset = []\n",
    "all_val_dataset = []\n",
    "all_test_dataset = []\n",
    "sum_class = 0\n",
    "\n",
    "for data_flag in data_flags:\n",
    "    \n",
    "    info = INFO[data_flag]\n",
    "    task = info['task']\n",
    "    n_channels = info['n_channels']\n",
    "    n_classes = len(info['label'])\n",
    "    \n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "    \n",
    "    train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "    val_dataset = DataClass(split='val', transform=data_transform, download=False)\n",
    "    test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "    print(train_dataset)\n",
    "    \n",
    "    for img, label in train_dataset:\n",
    "        all_training_dataset.append((img,label.item() + sum_class))\n",
    "    for img, label in val_dataset:\n",
    "        all_val_dataset.append((img,label.item() + sum_class))\n",
    "    if data_flag == 'dermamnist':\n",
    "        for img, label in test_dataset:\n",
    "            all_test_dataset.append((img,label.item() + sum_class))\n",
    "    sum_class += n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset, sampler, dataloader\n",
    "from pathlib import Path\n",
    "\n",
    "NUM_TRAIN_CLASSES = 9\n",
    "NUM_VAL_CLASSES = 7\n",
    "NUM_TEST_CLASSES = 7\n",
    "NUM_SAMPLES_PER_CLASS = 30\n",
    "\n",
    "class MedMNISTDataset(dataset.Dataset):\n",
    "\n",
    "    def __init__(self, split, num_support, num_query):\n",
    "        super().__init__()\n",
    "\n",
    "        split_labels = None\n",
    "        if (split == 'train'):\n",
    "            split_labels = 'train_labels'\n",
    "            dataset = all_training_dataset\n",
    "        elif (split == 'val'):\n",
    "            split_labels = 'val_labels'\n",
    "            dataset = all_val_dataset\n",
    "        elif (split == 'test'):\n",
    "            split_labels = 'test_labels'\n",
    "            dataset = all_test_dataset\n",
    "\n",
    "        self.all_images = []\n",
    "        all_labels = []\n",
    "        for image,label in dataset:\n",
    "            all_labels.append(label)\n",
    "            self.all_images.append(image)\n",
    "\n",
    "        num_classes = len(np.unique(all_labels))\n",
    "            \n",
    "        self.images_indices = [None] * num_classes\n",
    "        for i in range(num_classes):\n",
    "            self.images_indices[i] = []\n",
    "            \n",
    "        for i, elem in enumerate(all_labels):\n",
    "            if elem >= num_classes: continue\n",
    "            self.images_indices[elem].append(i)\n",
    "        \n",
    "        # shuffle characters\n",
    "        for i in range(num_classes):\n",
    "            np.random.default_rng(0).shuffle(self.images_indices[i])\n",
    "\n",
    "        # check problem arguments\n",
    "        assert num_support + num_query <= NUM_SAMPLES_PER_CLASS\n",
    "        self._num_support = num_support\n",
    "        self._num_query = num_query\n",
    "        self.split = split\n",
    "\n",
    "    def __getitem__(self, class_idxs):\n",
    "        \"\"\"Constructs a task.\n",
    "\n",
    "        Data for each class is sampled uniformly at random without replacement.\n",
    "\n",
    "        Args:\n",
    "            class_idxs (tuple[int]): class indices that comprise the task\n",
    "\n",
    "        Returns:\n",
    "            images_support (Tensor): task support images\n",
    "                shape (num_way * num_support, channels, height, width)\n",
    "            labels_support (Tensor): task support labels\n",
    "                shape (num_way * num_support,)\n",
    "            images_query (Tensor): task query images\n",
    "                shape (num_way * num_query, channels, height, width)\n",
    "            labels_query (Tensor): task query labels\n",
    "                shape (num_way * num_query,)\n",
    "        \"\"\"\n",
    "        images_support, images_query = [], []\n",
    "        labels_support, labels_query = [], []\n",
    "        \n",
    "        for label, class_idx in enumerate(class_idxs):\n",
    "            replace = False\n",
    "            if self.split == 'val' or self.split == 'test':\n",
    "                replace = True\n",
    "            # get a class's examples and sample from them\n",
    "            images_indices_label = np.random.default_rng().choice(\n",
    "                self.images_indices[class_idx],\n",
    "                size=self._num_support + self._num_query,\n",
    "                replace=replace\n",
    "            )\n",
    "\n",
    "            images = []\n",
    "            for index in images_indices_label:\n",
    "                images.append(self.all_images[index])\n",
    "            \n",
    "            # split sampled examples into support and query\n",
    "            images_support.extend(images[:self._num_support])\n",
    "            images_query.extend(images[self._num_support:])\n",
    "            labels_support.extend([label] * self._num_support)\n",
    "            labels_query.extend([label] * self._num_query)\n",
    "\n",
    "        # aggregate into tensors\n",
    "        images_support = torch.stack(images_support)  # shape (N*S, C, H, W)\n",
    "        labels_support = torch.tensor(labels_support)  # shape (N*S)\n",
    "        images_query = torch.stack(images_query)\n",
    "        labels_query = torch.tensor(labels_query)\n",
    "\n",
    "        return images_support, labels_support, images_query, labels_query\n",
    "\n",
    "\n",
    "class MedMNISTSampler(sampler.Sampler):\n",
    "    \"\"\"Samples task specification keys for an MedMNISTDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, split_idxs, num_way, num_tasks):\n",
    "        \"\"\"Inits OmniglotSampler.\n",
    "\n",
    "        Args:\n",
    "            split_idxs (range): indices that comprise the\n",
    "                training/validation/test split\n",
    "            num_way (int): number of classes per task\n",
    "            num_tasks (int): number of tasks to sample\n",
    "        \"\"\"\n",
    "        super().__init__(None)\n",
    "        self._split_idxs = split_idxs\n",
    "        self._num_way = num_way\n",
    "        self._num_tasks = num_tasks\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (\n",
    "            np.random.default_rng().choice(\n",
    "                self._split_idxs,\n",
    "                size=self._num_way,\n",
    "                replace=False\n",
    "            ) for _ in range(self._num_tasks)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._num_tasks\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_medmnist_dataloader(\n",
    "        split,\n",
    "        batch_size,\n",
    "        num_way,\n",
    "        num_support,\n",
    "        num_query,\n",
    "        num_tasks_per_epoch,\n",
    "        num_workers=2,\n",
    "):\n",
    "    \"\"\"Returns a dataloader.DataLoader for Omniglot.\n",
    "\n",
    "    Args:\n",
    "        split (str): one of 'train', 'val', 'test'\n",
    "        batch_size (int): number of tasks per batch\n",
    "        num_way (int): number of classes per task\n",
    "        num_support (int): number of support examples per class\n",
    "        num_query (int): number of query examples per class\n",
    "        num_tasks_per_epoch (int): number of tasks before DataLoader is\n",
    "            exhausted\n",
    "    \"\"\"\n",
    "    if split == 'train':\n",
    "        split_idxs = range(NUM_TRAIN_CLASSES)\n",
    "    elif split == 'val':\n",
    "        split_idxs = range(\n",
    "            NUM_TRAIN_CLASSES,\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES\n",
    "        )\n",
    "    elif split == 'test':\n",
    "        split_idxs = range(\n",
    "            NUM_TEST_CLASSES\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return dataloader.DataLoader(\n",
    "        dataset=MedMNISTDataset(split, num_support, num_query),\n",
    "        batch_size=batch_size,\n",
    "        sampler=MedMNISTSampler(split_idxs, num_way, num_tasks_per_epoch),\n",
    "        num_workers=num_workers,\n",
    "        # multiprocessing_context=\"fork\",\n",
    "        collate_fn=identity,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MedMNISTDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m dataloader_meta_train \u001b[38;5;241m=\u001b[39m get_medmnist_dataloader(\n\u001b[1;32m      5\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_meta_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks:\n\u001b[1;32m     16\u001b[0m     images_support, labels_support, images_query, labels_query \u001b[38;5;241m=\u001b[39m task\n",
      "File \u001b[0;32m~/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/cs330_project_hw2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1039\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Test custom dataloader\n",
    "# num_workers = 10\n",
    "\n",
    "# dataloader_meta_train = get_medmnist_dataloader(\n",
    "#         split='train',\n",
    "#         batch_size=8,\n",
    "#         num_way=7,\n",
    "#         num_support=5,\n",
    "#         num_query=15,\n",
    "#         num_tasks_per_epoch=240,\n",
    "#         num_workers=num_workers,\n",
    "#     )\n",
    "\n",
    "# tasks = next(iter(dataloader_meta_train))\n",
    "# for task in tasks:\n",
    "#     images_support, labels_support, images_query, labels_query = task\n",
    "#     print(images_support.shape)\n",
    "#     print(labels_support.shape)\n",
    "#     print(images_query.shape)\n",
    "#     print(labels_query.shape)\n",
    "#     print(labels_support)\n",
    "#     print(labels_query)\n",
    "#     break\n",
    "\n",
    "# dataloader_meta_val = get_medmnist_dataloader(\n",
    "#         split='val',\n",
    "#         batch_size=8,\n",
    "#         num_way=7,\n",
    "#         num_support=5,\n",
    "#         num_query=15,\n",
    "#         num_tasks_per_epoch=240,\n",
    "#         num_workers=num_workers,\n",
    "#     )\n",
    "\n",
    "# tasks = next(iter(dataloader_meta_val))\n",
    "# for task in tasks:\n",
    "#     images_support, labels_support, images_query, labels_query = task\n",
    "#     print(images_support.shape)\n",
    "#     print(labels_support.shape)\n",
    "#     print(images_query.shape)\n",
    "#     print(labels_query.shape)\n",
    "#     print(labels_support)\n",
    "#     print(labels_query)\n",
    "#     break\n",
    "\n",
    "# dataloader_meta_test = get_medmnist_dataloader(\n",
    "#         split='test',\n",
    "#         batch_size=1,\n",
    "#         num_way=7,\n",
    "#         num_support=5,\n",
    "#         num_query=15,\n",
    "#         num_tasks_per_epoch=240,\n",
    "#         num_workers=num_workers,\n",
    "#     )\n",
    "\n",
    "# tasks = next(iter(dataloader_meta_test))\n",
    "# for task in tasks:\n",
    "#     images_support, labels_support, images_query, labels_query = task\n",
    "#     print(images_support.shape)\n",
    "#     print(labels_support.shape)\n",
    "#     print(images_query.shape)\n",
    "#     print(labels_query.shape)\n",
    "#     print(labels_support)\n",
    "#     print(labels_query)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, we define a simple model for illustration, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(logits, labels):\n",
    "    \"\"\"Returns the mean accuracy of a model's predictions on a set of examples.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): model predicted logits\n",
    "            shape (examples, classes)\n",
    "        labels (torch.Tensor): classification labels from 0 to num_classes - 1\n",
    "            shape (examples,)\n",
    "    \"\"\"\n",
    "\n",
    "    assert logits.dim() == 2\n",
    "    assert labels.dim() == 1\n",
    "    assert logits.shape[0] == labels.shape[0]\n",
    "    y = torch.argmax(logits, dim=-1) == labels\n",
    "    y = y.type(torch.float)\n",
    "    return torch.mean(y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 3\n",
    "NUM_HIDDEN_CHANNELS = 32\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CONV_LAYERS = 8\n",
    "SUMMARY_INTERVAL = 10\n",
    "SAVE_INTERVAL = 100\n",
    "LOG_INTERVAL = 10\n",
    "VAL_INTERVAL = LOG_INTERVAL * 5\n",
    "NUM_TEST_TASKS = 100\n",
    "\n",
    "class MAML:\n",
    "    \"\"\"Trains and assesses a MAML.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_outputs,\n",
    "            num_inner_steps,\n",
    "            inner_lr,\n",
    "            learn_inner_lrs,\n",
    "            outer_lr,\n",
    "            log_dir,\n",
    "            device\n",
    "    ):\n",
    "        \"\"\"Inits MAML.\n",
    "\n",
    "        The network consists of four convolutional blocks followed by a linear\n",
    "        head layer. Each convolutional block comprises a convolution layer, a\n",
    "        batch normalization layer, and ReLU activation.\n",
    "\n",
    "        Note that unlike conventional use, batch normalization is always done\n",
    "        with batch statistics, regardless of whether we are training or\n",
    "        evaluating. This technically makes meta-learning transductive, as\n",
    "        opposed to inductive.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): dimensionality of output, i.e. number of classes\n",
    "                in a task\n",
    "            num_inner_steps (int): number of inner-loop optimization steps\n",
    "            inner_lr (float): learning rate for inner-loop optimization\n",
    "                If learn_inner_lrs=True, inner_lr serves as the initialization\n",
    "                of the learning rates.\n",
    "            learn_inner_lrs (bool): whether to learn the above\n",
    "            outer_lr (float): learning rate for outer-loop optimization\n",
    "            log_dir (str): path to logging directory\n",
    "            device (str): device to be used\n",
    "        \"\"\"\n",
    "        meta_parameters = {}\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # construct feature extractor\n",
    "        in_channels = NUM_INPUT_CHANNELS\n",
    "        for i in range(NUM_CONV_LAYERS):\n",
    "            meta_parameters[f'conv{i}'] = nn.init.xavier_uniform_(\n",
    "                torch.empty(\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    in_channels,\n",
    "                    KERNEL_SIZE,\n",
    "                    KERNEL_SIZE,\n",
    "                    requires_grad=True,\n",
    "                    device=self.device\n",
    "                )\n",
    "            )\n",
    "            meta_parameters[f'b{i}'] = nn.init.zeros_(\n",
    "                torch.empty(\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    requires_grad=True,\n",
    "                    device=self.device\n",
    "                )\n",
    "            )\n",
    "            in_channels = NUM_HIDDEN_CHANNELS\n",
    "\n",
    "        # construct linear head layer\n",
    "        meta_parameters[f'w{NUM_CONV_LAYERS}'] = nn.init.xavier_uniform_(\n",
    "            torch.empty(\n",
    "                num_outputs,\n",
    "                NUM_HIDDEN_CHANNELS,\n",
    "                requires_grad=True,\n",
    "                device=self.device\n",
    "            )\n",
    "        )\n",
    "        meta_parameters[f'b{NUM_CONV_LAYERS}'] = nn.init.zeros_(\n",
    "            torch.empty(\n",
    "                num_outputs,\n",
    "                requires_grad=True,\n",
    "                device=self.device\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._meta_parameters = meta_parameters\n",
    "        self._num_inner_steps = num_inner_steps\n",
    "        self._inner_lrs = {\n",
    "            k: torch.tensor(inner_lr, requires_grad=learn_inner_lrs)\n",
    "            for k in self._meta_parameters.keys()\n",
    "        }\n",
    "        self._outer_lr = outer_lr\n",
    "\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            list(self._meta_parameters.values()) +\n",
    "            list(self._inner_lrs.values()),\n",
    "            lr=self._outer_lr\n",
    "        )\n",
    "        self._log_dir = log_dir\n",
    "        os.makedirs(self._log_dir, exist_ok=True)\n",
    "\n",
    "        self._start_train_step = 0\n",
    "\n",
    "    def _forward(self, images, parameters):\n",
    "        \"\"\"Computes predicted classification logits.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): batch of Omniglot images\n",
    "                shape (num_images, channels, height, width)\n",
    "            parameters (dict[str, Tensor]): parameters to use for\n",
    "                the computation\n",
    "\n",
    "        Returns:\n",
    "            a Tensor consisting of a batch of logits\n",
    "                shape (num_images, classes)\n",
    "        \"\"\"\n",
    "        x = images\n",
    "        for i in range(NUM_CONV_LAYERS):\n",
    "            x = F.conv2d(\n",
    "                input=x,\n",
    "                weight=parameters[f'conv{i}'],\n",
    "                bias=parameters[f'b{i}'],\n",
    "                stride=1,\n",
    "                padding='same'\n",
    "            )\n",
    "            x = F.batch_norm(x, None, None, training=True)\n",
    "            x = F.leaky_relu(x)\n",
    "        x = torch.mean(x, dim=[2, 3])\n",
    "        return F.linear(\n",
    "            input=x,\n",
    "            weight=parameters[f'w{NUM_CONV_LAYERS}'],\n",
    "            bias=parameters[f'b{NUM_CONV_LAYERS}']\n",
    "        )\n",
    "\n",
    "    def _inner_loop(self, images, labels, train):\n",
    "        \"\"\"Computes the adapted network parameters via the MAML inner loop.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): task support set inputs\n",
    "                shape (num_images, channels, height, width)\n",
    "            labels (Tensor): task support set outputs\n",
    "                shape (num_images,)\n",
    "            train (bool): whether we are training or evaluating\n",
    "\n",
    "        Returns:\n",
    "            parameters (dict[str, Tensor]): adapted network parameters\n",
    "            accuracies (list[float]): support set accuracy over the course of\n",
    "                the inner loop, length num_inner_steps + 1\n",
    "            gradients(list[float]): gradients computed from auto.grad, just needed\n",
    "                for autograders, no need to use this value in your code and feel to replace\n",
    "                with underscore       \n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        parameters = {\n",
    "            k: torch.clone(v)\n",
    "            for k, v in self._meta_parameters.items()\n",
    "        }\n",
    "        gradients = None\n",
    "        ### START CODE HERE ###\n",
    "        # TODO: finish implementing this method.\n",
    "        # This method computes the inner loop (adaptation) procedure\n",
    "        # over the course of _num_inner_steps steps for one\n",
    "        # task. It also scores the model along the way.\n",
    "        # Make sure to populate accuracies and update parameters.\n",
    "        # Use F.cross_entropy to compute classification losses.\n",
    "        # Use util.score to compute accuracies.\n",
    "        for _ in range(self._num_inner_steps):\n",
    "            support_features = self._forward(images, parameters=parameters)\n",
    "            loss = F.cross_entropy(support_features, labels.squeeze())\n",
    "\n",
    "            if train:\n",
    "                gradients = autograd.grad(loss, parameters.values(), create_graph=True)\n",
    "            else:\n",
    "                gradients = autograd.grad(loss, parameters.values(), create_graph=False)\n",
    "\n",
    "            layers = list(parameters.keys())\n",
    "            for i in range(len(layers)): parameters[layers[i]] = parameters[layers[i]] - self._inner_lrs[layers[i]] * gradients[i]\n",
    "            accuracies.append(score(support_features, labels.squeeze()))\n",
    "\n",
    "        # Batch accuracy\n",
    "        batch_features = self._forward(images, parameters)\n",
    "        accuracies.append(score(batch_features, labels.squeeze()))\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        return parameters, accuracies, gradients\n",
    "\n",
    "    def _outer_step(self, task_batch, train):\n",
    "        \"\"\"Computes the MAML loss and metrics on a batch of tasks.\n",
    "\n",
    "        Args:\n",
    "            task_batch (tuple): batch of tasks from an Omniglot DataLoader\n",
    "            train (bool): whether we are training or evaluating\n",
    "\n",
    "        Returns:\n",
    "            outer_loss (Tensor): mean MAML loss over the batch, scalar\n",
    "            accuracies_support (ndarray): support set accuracy over the\n",
    "                course of the inner loop, averaged over the task batch\n",
    "                shape (num_inner_steps + 1,)\n",
    "            accuracy_query (float): query set accuracy of the adapted\n",
    "                parameters, averaged over the task batch\n",
    "        \"\"\"\n",
    "        outer_loss_batch = []\n",
    "        accuracies_support_batch = []\n",
    "        accuracy_query_batch = []\n",
    "        for i, task in enumerate(task_batch):\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "            images_support = images_support.to(self.device)\n",
    "            labels_support = labels_support.to(self.device)\n",
    "            images_query = images_query.to(self.device)\n",
    "            labels_query = labels_query.to(self.device)\n",
    "\n",
    "            parameters, support_accuraccies, _ = self._inner_loop(images_support, labels_support, train)\n",
    "\n",
    "            query_features = self._forward(images_query, parameters)\n",
    "            loss = F.cross_entropy(query_features, labels_query.squeeze())\n",
    "            outer_loss_batch.append(loss)\n",
    "\n",
    "            accuracies_support_batch.append(support_accuraccies)\n",
    "            query_accuracies = score(query_features, labels_query.squeeze())\n",
    "            accuracy_query_batch.append(query_accuracies)\n",
    "            ### END CODE HERE ###\n",
    "        outer_loss = torch.mean(torch.stack(outer_loss_batch))\n",
    "        accuracies_support = np.mean(\n",
    "            accuracies_support_batch,\n",
    "            axis=0\n",
    "        )\n",
    "        accuracy_query = np.mean(accuracy_query_batch)\n",
    "        return outer_loss, accuracies_support, accuracy_query\n",
    "\n",
    "    def train(self, dataloader_meta_train, dataloader_meta_val, writer):\n",
    "        \"\"\"Train the MAML.\n",
    "\n",
    "        Consumes dataloader_meta_train to optimize MAML meta-parameters\n",
    "        while periodically validating on dataloader_meta_val, logging metrics, and\n",
    "        saving checkpoints.\n",
    "\n",
    "        Args:\n",
    "            dataloader_meta_train (DataLoader): loader for train tasks\n",
    "            dataloader_meta_val (DataLoader): loader for validation tasks\n",
    "            writer (SummaryWriter): TensorBoard logger\n",
    "        \"\"\"\n",
    "        print(f'Starting training at iteration {self._start_train_step}.')\n",
    "        for i_step, task_batch in enumerate(\n",
    "                dataloader_meta_train,\n",
    "                start=self._start_train_step\n",
    "        ):\n",
    "            self._optimizer.zero_grad()\n",
    "            outer_loss, accuracies_support, accuracy_query = (                self._outer_step(task_batch, train=True)\n",
    "            )\n",
    "            outer_loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            if i_step % LOG_INTERVAL == 0:\n",
    "                print(\n",
    "                    f'Iteration {i_step}: '\n",
    "                    f'loss: {outer_loss.item():.3f}, '\n",
    "                    f'pre-adaptation support accuracy: '\n",
    "                    f'{accuracies_support[0]:.3f}, '\n",
    "                    f'post-adaptation support accuracy: '\n",
    "                    f'{accuracies_support[-1]:.3f}, '\n",
    "                    f'post-adaptation query accuracy: '\n",
    "                    f'{accuracy_query:.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/train', outer_loss.item(), i_step)\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/pre_adapt_support',\n",
    "                    accuracies_support[0],\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/post_adapt_support',\n",
    "                    accuracies_support[-1],\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/post_adapt_query',\n",
    "                    accuracy_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % VAL_INTERVAL == 0:\n",
    "                losses = []\n",
    "                accuracies_pre_adapt_support = []\n",
    "                accuracies_post_adapt_support = []\n",
    "                accuracies_post_adapt_query = []\n",
    "                for val_task_batch in dataloader_meta_val:\n",
    "                    outer_loss, accuracies_support, accuracy_query = (\n",
    "                        self._outer_step(val_task_batch, train=False)\n",
    "                    )\n",
    "                    if (math.isnan(outer_loss.item())): break\n",
    "                    losses.append(outer_loss.item())\n",
    "                    accuracies_pre_adapt_support.append(accuracies_support[0])\n",
    "                    accuracies_post_adapt_support.append(accuracies_support[-1])\n",
    "                    accuracies_post_adapt_query.append(accuracy_query)\n",
    "                loss = np.mean(losses)\n",
    "                accuracy_pre_adapt_support = np.mean(\n",
    "                    accuracies_pre_adapt_support\n",
    "                )\n",
    "                accuracy_post_adapt_support = np.mean(\n",
    "                    accuracies_post_adapt_support\n",
    "                )\n",
    "                accuracy_post_adapt_query = np.mean(\n",
    "                    accuracies_post_adapt_query\n",
    "                )\n",
    "                print(\n",
    "                    f'Validation: '\n",
    "                    f'loss: {loss:.3f}, '\n",
    "                    f'pre-adaptation support accuracy: '\n",
    "                    f'{accuracy_pre_adapt_support:.3f}, '\n",
    "                    f'post-adaptation support accuracy: '\n",
    "                    f'{accuracy_post_adapt_support:.3f}, '\n",
    "                    f'post-adaptation query accuracy: '\n",
    "                    f'{accuracy_post_adapt_query:.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/val', loss, i_step)\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/pre_adapt_support',\n",
    "                    accuracy_pre_adapt_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/post_adapt_support',\n",
    "                    accuracy_post_adapt_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/post_adapt_query',\n",
    "                    accuracy_post_adapt_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % SAVE_INTERVAL == 0:\n",
    "                self._save(i_step)\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        \"\"\"Evaluate the MAML on test tasks.\n",
    "\n",
    "        Args:\n",
    "            dataloader_test (DataLoader): loader for test tasks\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for task_batch in dataloader_test:\n",
    "            _, _, accuracy_query = self._outer_step(task_batch, train=False)\n",
    "            if (math.isnan(accuracy_query.item())): break\n",
    "            accuracies.append(accuracy_query)\n",
    "        mean = np.mean(accuracies)\n",
    "        std = np.std(accuracies)\n",
    "        mean_95_confidence_interval = 1.96 * std / np.sqrt(NUM_TEST_TASKS)\n",
    "        print(\n",
    "            f'Accuracy over {NUM_TEST_TASKS} test tasks: '\n",
    "            f'mean {mean:.3f}, '\n",
    "            f'95% confidence interval {mean_95_confidence_interval:.3f}'\n",
    "        )\n",
    "\n",
    "    def load(self, checkpoint_step):\n",
    "        \"\"\"Loads a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration of checkpoint to load\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if checkpoint for checkpoint_step is not found\n",
    "        \"\"\"\n",
    "        target_path = (\n",
    "            f'{os.path.join(self._log_dir, \"state\")}'\n",
    "            f'{checkpoint_step}.pt'\n",
    "        )\n",
    "        if os.path.isfile(target_path):\n",
    "            state = torch.load(target_path)\n",
    "            self._meta_parameters = state['meta_parameters']\n",
    "            self._inner_lrs = state['inner_lrs']\n",
    "            self._optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            self._start_train_step = checkpoint_step + 1\n",
    "            print(f'Loaded checkpoint iteration {checkpoint_step}.')\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'No checkpoint for iteration {checkpoint_step} found.'\n",
    "            )\n",
    "\n",
    "    def _save(self, checkpoint_step):\n",
    "        \"\"\"Saves parameters and optimizer state_dict as a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration to label checkpoint with\n",
    "        \"\"\"\n",
    "        optimizer_state_dict = self._optimizer.state_dict()\n",
    "        torch.save(\n",
    "            dict(meta_parameters=self._meta_parameters,\n",
    "                 inner_lrs=self._inner_lrs,\n",
    "                 optimizer_state_dict=optimizer_state_dict),\n",
    "            f'{os.path.join(self._log_dir, \"state\")}{checkpoint_step}.pt'\n",
    "        )\n",
    "        print('Saved checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "# DEVICE = \"mps\"\n",
    "num_workers = 0\n",
    "if platform == \"linux\":\n",
    "    DEVICE = \"cuda\"\n",
    "    num_workers = 10\n",
    "\n",
    "log_dir = None\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "Starting training at iteration 0.\n",
      "Iteration 0: loss: 3.982, pre-adaptation support accuracy: 0.146, post-adaptation support accuracy: 0.132, post-adaptation query accuracy: 0.137\n",
      "Validation: loss: 3.323, pre-adaptation support accuracy: 0.143, post-adaptation support accuracy: 0.148, post-adaptation query accuracy: 0.144\n",
      "Saved checkpoint.\n",
      "Iteration 10: loss: 2.232, pre-adaptation support accuracy: 0.136, post-adaptation support accuracy: 0.186, post-adaptation query accuracy: 0.195\n",
      "Iteration 20: loss: 2.030, pre-adaptation support accuracy: 0.132, post-adaptation support accuracy: 0.211, post-adaptation query accuracy: 0.198\n",
      "Iteration 30: loss: 2.206, pre-adaptation support accuracy: 0.146, post-adaptation support accuracy: 0.161, post-adaptation query accuracy: 0.145\n",
      "Iteration 40: loss: 1.923, pre-adaptation support accuracy: 0.150, post-adaptation support accuracy: 0.271, post-adaptation query accuracy: 0.252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m\n\u001b[1;32m     23\u001b[0m dataloader_meta_train \u001b[38;5;241m=\u001b[39m get_medmnist_dataloader(\n\u001b[1;32m     24\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m dataloader_meta_val \u001b[38;5;241m=\u001b[39m get_medmnist_dataloader(\n\u001b[1;32m     34\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \u001b[38;5;66;03m#2\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m     41\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[43mmaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_meta_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_meta_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m dataloader_meta_test \u001b[38;5;241m=\u001b[39m get_medmnist_dataloader(\n\u001b[1;32m     50\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     51\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     59\u001b[0m maml\u001b[38;5;241m.\u001b[39mtest(dataloader_meta_test)\n",
      "Cell \u001b[0;32mIn[11], line 252\u001b[0m, in \u001b[0;36mMAML.train\u001b[0;34m(self, dataloader_meta_train, dataloader_meta_val, writer)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_step, task_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    248\u001b[0m         dataloader_meta_train,\n\u001b[1;32m    249\u001b[0m         start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_train_step\n\u001b[1;32m    250\u001b[0m ):\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 252\u001b[0m     outer_loss, accuracies_support, accuracy_query \u001b[38;5;241m=\u001b[39m (                \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_outer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     outer_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[11], line 216\u001b[0m, in \u001b[0;36mMAML._outer_step\u001b[0;34m(self, task_batch, train)\u001b[0m\n\u001b[1;32m    213\u001b[0m images_query \u001b[38;5;241m=\u001b[39m images_query\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    214\u001b[0m labels_query \u001b[38;5;241m=\u001b[39m labels_query\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 216\u001b[0m parameters, support_accuraccies, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m query_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(images_query, parameters)\n\u001b[1;32m    219\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(query_features, labels_query\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "Cell \u001b[0;32mIn[11], line 185\u001b[0m, in \u001b[0;36mMAML._inner_loop\u001b[0;34m(self, images, labels, train)\u001b[0m\n\u001b[1;32m    182\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(score(support_features, labels\u001b[38;5;241m.\u001b[39msqueeze()))\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Batch accuracy\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(score(batch_features, labels\u001b[38;5;241m.\u001b[39msqueeze()))\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 123\u001b[0m, in \u001b[0;36mMAML._forward\u001b[0;34m(self, images, parameters)\u001b[0m\n\u001b[1;32m    121\u001b[0m x \u001b[38;5;241m=\u001b[39m images\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_CONV_LAYERS):\n\u001b[0;32m--> 123\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbatch_norm(x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if log_dir is None:\n",
    "    log_dir = f'./logs/maml-multiple/'  # pylint: disable=line-too-long\n",
    "writer = tensorboard.SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "num_way = 7\n",
    "num_inner_steps = 1\n",
    "inner_lr = 15.0\n",
    "learn_inner_lrs = True\n",
    "learning_rate = 0.01\n",
    "\n",
    "maml = MAML(\n",
    "    num_way,\n",
    "    num_inner_steps,\n",
    "    inner_lr,\n",
    "    learn_inner_lrs,\n",
    "    learning_rate,\n",
    "    log_dir,\n",
    "    DEVICE\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "num_way = 7\n",
    "num_support = 5\n",
    "num_query = 15\n",
    "num_training_tasks = 8000\n",
    "\n",
    "dataloader_meta_train = get_medmnist_dataloader(\n",
    "        split='train',\n",
    "        batch_size=batch_size,\n",
    "        num_way=num_way,\n",
    "        num_support=num_support,\n",
    "        num_query=num_query,\n",
    "        num_tasks_per_epoch=num_training_tasks,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "dataloader_meta_val = get_medmnist_dataloader(\n",
    "        split='val',\n",
    "        batch_size=batch_size, #2\n",
    "        num_way=num_way,\n",
    "        num_support=num_support,\n",
    "        num_query=num_query, \n",
    "        num_tasks_per_epoch=num_training_tasks, # 2\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "maml.train(\n",
    "    dataloader_meta_train,\n",
    "    dataloader_meta_val,\n",
    "    writer\n",
    ")\n",
    "\n",
    "dataloader_meta_test = get_medmnist_dataloader(\n",
    "        split='test',\n",
    "        batch_size=1,\n",
    "        num_way=num_way,\n",
    "        num_support=num_support,\n",
    "        num_query=num_query,\n",
    "        num_tasks_per_epoch=NUM_TEST_TASKS,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "maml.test(dataloader_meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 3\n",
    "NUM_HIDDEN_CHANNELS = 64\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CONV_LAYERS = 4\n",
    "SUMMARY_INTERVAL = 10\n",
    "SAVE_INTERVAL = 100\n",
    "PRINT_INTERVAL = 10\n",
    "VAL_INTERVAL = PRINT_INTERVAL * 5\n",
    "NUM_TEST_TASKS = 100\n",
    "\n",
    "class ProtoNetNetwork(nn.Module):\n",
    "    \"\"\"Container for ProtoNet weights and image-to-latent computation.\"\"\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        \"\"\"Inits ProtoNetNetwork.\n",
    "\n",
    "        The network consists of four convolutional blocks, each comprising a\n",
    "        convolution layer, a batch normalization layer, ReLU activation, and 2x2\n",
    "        max pooling for downsampling. There is an additional flattening\n",
    "        operation at the end.\n",
    "\n",
    "        Note that unlike conventional use, batch normalization is always done\n",
    "        with batch statistics, regardless of whether we are training or\n",
    "        evaluating. This technically makes meta-learning transductive, as\n",
    "        opposed to inductive.\n",
    "\n",
    "        Args:\n",
    "            device (str): device to be used\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels = NUM_INPUT_CHANNELS\n",
    "        for _ in range(NUM_CONV_LAYERS):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    (KERNEL_SIZE, KERNEL_SIZE),\n",
    "                    padding='same'\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm2d(NUM_HIDDEN_CHANNELS))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = NUM_HIDDEN_CHANNELS\n",
    "        layers.append(nn.Flatten())\n",
    "        self._layers = nn.Sequential(*layers)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Computes the latent representation of a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): batch of Omniglot images\n",
    "                shape (num_images, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor containing a batch of latent representations\n",
    "                shape (num_images, latents)\n",
    "        \"\"\"\n",
    "        return self._layers(images)\n",
    "\n",
    "\n",
    "class ProtoNet:\n",
    "    \"\"\"Trains and assesses a prototypical network.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate, log_dir, device, compile=False, backend=None, learner=None, val_interval=None, save_interval=None, bio=False):\n",
    "        \"\"\"Inits ProtoNet.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): learning rate for the Adam optimizer\n",
    "            log_dir (str): path to logging directory\n",
    "            device (str): device to be used\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        if learner is None:\n",
    "            self._network = ProtoNetNetwork(device)\n",
    "        else: \n",
    "            self._network = learner.to(device)\n",
    "\n",
    "        self.val_interval = VAL_INTERVAL if val_interval is None else val_interval\n",
    "        self.save_interval = SAVE_INTERVAL if save_interval is None else save_interval\n",
    "        self.bio = bio\n",
    "\n",
    "        if(compile == True):\n",
    "            try:\n",
    "                self._network = torch.compile(self._network, backend=backend)\n",
    "                print(f\"ProtoNetNetwork model compiled\")\n",
    "            except Exception as err:\n",
    "                print(f\"Model compile not supported: {err}\")\n",
    "\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._network.parameters(),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "        self._log_dir = log_dir\n",
    "        os.makedirs(self._log_dir, exist_ok=True)\n",
    "\n",
    "        self._start_train_step = 0\n",
    "\n",
    "    def _step(self, task_batch):\n",
    "        \"\"\"Computes ProtoNet mean loss (and accuracy) on a batch of tasks.\n",
    "\n",
    "        Args:\n",
    "            task_batch (tuple[Tensor, Tensor, Tensor, Tensor]):\n",
    "                batch of tasks from an Omniglot DataLoader\n",
    "\n",
    "        Returns:\n",
    "            a Tensor containing mean ProtoNet loss over the batch\n",
    "                shape ()\n",
    "            mean support set accuracy over the batch as a float\n",
    "            mean query set accuracy over the batch as a float\n",
    "        \"\"\"\n",
    "        loss_batch = []\n",
    "        accuracy_support_batch = []\n",
    "        accuracy_query_batch = []\n",
    "        for i, task in enumerate(task_batch):\n",
    "            # print(i)\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "            images_support = images_support.to(self.device)\n",
    "            labels_support = labels_support.to(self.device)\n",
    "            images_query = images_query.to(self.device)\n",
    "            labels_query = labels_query.to(self.device)\n",
    "\n",
    "            ### START CODE HERE ###\n",
    "            # TODO: finish implementing this method.\n",
    "            # For a given task, compute the prototypes and the protonet loss.\n",
    "            # Use F.cross_entropy to compute classification losses.\n",
    "            # Use util.score to compute accuracies.\n",
    "            # Make sure to populate loss_batch, accuracy_support_batch, and\n",
    "            # accuracy_query_batch.\n",
    "            loss_batch, accuracy_support_batch, accuracy_query_batch\n",
    "            # compute prototypes without tracking gradients\n",
    "                # Generate support features\n",
    "            support_features = self._network.forward(images_support)\n",
    "            # Calculate prototypes from features\n",
    "            prototypes = []\n",
    "            n  = torch.unique(labels_support).shape[-1]\n",
    "            for i in range(n):\n",
    "                indices = (labels_support == i).nonzero()\n",
    "                class_features = support_features[indices]\n",
    "                with torch.no_grad():\n",
    "                    prototypes.append(torch.mean(class_features, dim=0))\n",
    "                # Calculate features for support\n",
    "            prototypes = torch.cat(prototypes)\n",
    "            support_features = torch.stack([support_features] * prototypes.size(0), dim=1)\n",
    "            supp_diffs = torch.sub(support_features, prototypes)\n",
    "            supp_sq_norms = torch.linalg.norm(supp_diffs,ord=2,dim=2).square()\n",
    "            accuracy_support_batch.append(score(-supp_sq_norms, labels_support))\n",
    "\n",
    "            # Generate query features\n",
    "            query_features = self._network.forward(images_query)\n",
    "            query_features_batch = torch.stack([query_features] * prototypes.size(0), dim=1)\n",
    "            query_diffs = torch.sub(query_features_batch, prototypes)\n",
    "            query_sq_norms = torch.linalg.norm(query_diffs,ord=2,dim=2).square()\n",
    "            accuracy_query_batch.append(score(-query_sq_norms, labels_query))\n",
    "            # Compute loss\n",
    "            loss_batch.append(F.cross_entropy(-query_sq_norms, labels_query))\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "        return (\n",
    "            torch.mean(torch.stack(loss_batch)),\n",
    "            np.mean(accuracy_support_batch),\n",
    "            np.mean(accuracy_query_batch)\n",
    "        )\n",
    "\n",
    "    def train(self, dataloader_meta_train, dataloader_meta_val, writer):\n",
    "        \"\"\"Train the ProtoNet.\n",
    "\n",
    "        Consumes dataloader_meta_train to optimize weights of ProtoNetNetwork\n",
    "        while periodically validating on dataloader_meta_val, logging metrics, and\n",
    "        saving checkpoints.\n",
    "\n",
    "        Args:\n",
    "            dataloader_meta_train (DataLoader): loader for train tasks\n",
    "            dataloader_meta_val (DataLoader): loader for validation tasks\n",
    "            writer (SummaryWriter): TensorBoard logger\n",
    "        \"\"\"\n",
    "        print(f'Starting training at iteration {self._start_train_step}.')\n",
    "        MAX_TRAIN = len(dataloader_meta_train)\n",
    "        # exit()\n",
    "        for i_step, task_batch in enumerate(\n",
    "                dataloader_meta_train,\n",
    "                start=self._start_train_step\n",
    "        ):\n",
    "            if i_step > MAX_TRAIN:\n",
    "                break\n",
    "            self._optimizer.zero_grad()\n",
    "            loss, accuracy_support, accuracy_query = self._step(task_batch)\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            if i_step % PRINT_INTERVAL == 0:\n",
    "                print(\n",
    "                    f'Iteration {i_step}: '\n",
    "                    f'loss: {loss.item():.3f}, '\n",
    "                    f'support accuracy: {accuracy_support.item():.3f}, '\n",
    "                    f'query accuracy: {accuracy_query.item():.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/train', loss.item(), i_step)\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/support',\n",
    "                    accuracy_support.item(),\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/query',\n",
    "                    accuracy_query.item(),\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % self.val_interval == 0:\n",
    "                print(\"Start Validation...\")\n",
    "                with torch.no_grad():\n",
    "                    losses, accuracies_support, accuracies_query = [], [], []\n",
    "                    for i, val_task_batch in enumerate(dataloader_meta_val):\n",
    "                        if self.bio and i > 600:\n",
    "                            break\n",
    "                        loss, accuracy_support, accuracy_query = (\n",
    "                            self._step(val_task_batch)\n",
    "                        )\n",
    "                        losses.append(loss.item())\n",
    "                        accuracies_support.append(accuracy_support)\n",
    "                        accuracies_query.append(accuracy_query)\n",
    "                    loss = np.mean(losses)\n",
    "                    accuracy_support = np.mean(accuracies_support)\n",
    "                    accuracy_query = np.mean(accuracies_query)\n",
    "                    ci95 = 1.96 * np.std(accuracies_query) / np.sqrt(600 * 4)\n",
    "                if self.bio:\n",
    "                    print(\n",
    "                        f'Validation: '\n",
    "                        f'loss: {loss:.3f}, '\n",
    "                        f'support accuracy: {accuracy_support:.3f}, '\n",
    "                        f'query accuracy: {accuracy_query:.3f}',\n",
    "                        f'Ci95: {ci95:.3f}'\n",
    "                    )\n",
    "                else: \n",
    "                    print(\n",
    "                        f'Validation: '\n",
    "                        f'loss: {loss:.3f}, '\n",
    "                        f'support accuracy: {accuracy_support:.3f}, '\n",
    "                        f'query accuracy: {accuracy_query:.3f}'\n",
    "                    )\n",
    "                writer.add_scalar('loss/val', loss, i_step)\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/support',\n",
    "                    accuracy_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/query',\n",
    "                    accuracy_query,\n",
    "                    i_step\n",
    "                )\n",
    "                if self.bio:\n",
    "                    writer.add_scalar(\n",
    "                        'val_accuracy/ci95',\n",
    "                        ci95,\n",
    "                        i_step\n",
    "                    )\n",
    "            if i_step % self.save_interval == 0:\n",
    "                self._save(i_step)\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        \"\"\"Evaluate the ProtoNet on test tasks.\n",
    "\n",
    "        Args:\n",
    "            dataloader_test (DataLoader): loader for test tasks\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for i, task_batch in enumerate(dataloader_test):\n",
    "            accuracies.append(self._step(task_batch)[2])\n",
    "        mean = np.mean(accuracies)\n",
    "        std = np.std(accuracies)\n",
    "        mean_95_confidence_interval = 1.96 * std / np.sqrt(NUM_TEST_TASKS)\n",
    "        print(\n",
    "            f'Accuracy over {NUM_TEST_TASKS} test tasks: '\n",
    "            f'mean {mean:.3f}, '\n",
    "            f'95% confidence interval {mean_95_confidence_interval:.3f}'\n",
    "        )\n",
    "\n",
    "    def load(self, checkpoint_step, filename=\"\"):\n",
    "        \"\"\"Loads a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration of checkpoint to load\n",
    "            filename (str): directly setting name of checkpoint file, default =\"\", when argument is passed, then checkpoint will be ignored\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if checkpoint for checkpoint_step is not found\n",
    "        \"\"\"\n",
    "        target_path = (\n",
    "            f'{os.path.join(self._log_dir, \"state\")}'\n",
    "            f'{checkpoint_step}.pt'\n",
    "        ) if filename == \"\" else filename\n",
    "        if os.path.isfile(target_path):\n",
    "            state = torch.load(target_path)\n",
    "            self._network.load_state_dict(state['network_state_dict'])\n",
    "            self._optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            self._start_train_step = checkpoint_step + 1\n",
    "            print(f'Loaded checkpoint iteration {checkpoint_step}.')\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'No checkpoint for iteration {checkpoint_step} found.'\n",
    "            )\n",
    "\n",
    "    def _save(self, checkpoint_step):\n",
    "        \"\"\"Saves network and optimizer state_dicts as a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration to label checkpoint with\n",
    "        \"\"\"\n",
    "        torch.save(\n",
    "            dict(network_state_dict=self._network.state_dict(),\n",
    "                 optimizer_state_dict=self._optimizer.state_dict()),\n",
    "            f'{os.path.join(self._log_dir, \"state\")}{checkpoint_step}.pt'\n",
    "        )\n",
    "        print('Saved checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "backend = 'inductor'\n",
    "\n",
    "batch_size = 16\n",
    "num_way = 7\n",
    "num_support = 5\n",
    "num_query = 15\n",
    "num_training_tasks = 8000\n",
    "\n",
    "log_dir = f'./logs/protonet-multiple/'  # pylint: disable=line-too-long\n",
    "writer = tensorboard.SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "dataloader_meta_train = get_medmnist_dataloader(\n",
    "        split='train',\n",
    "        batch_size=batch_size,\n",
    "        num_way=num_way,\n",
    "        num_support=num_support,\n",
    "        num_query=num_query,\n",
    "        num_tasks_per_epoch=num_training_tasks,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "dataloader_meta_val = get_medmnist_dataloader(\n",
    "        split='val',\n",
    "        batch_size=batch_size, #2\n",
    "        num_way=num_way,\n",
    "        num_support=num_support,\n",
    "        num_query=num_query, \n",
    "        num_tasks_per_epoch=num_training_tasks, # 2\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "protonet = ProtoNet(learning_rate, log_dir, DEVICE, backend)\n",
    "\n",
    "protonet.train(\n",
    "        dataloader_meta_train,\n",
    "        dataloader_meta_val,\n",
    "        writer\n",
    "    )\n",
    "\n",
    "dataloader_meta_test = get_medmnist_dataloader(\n",
    "        split='test',\n",
    "        batch_size=1,\n",
    "        num_way=num_way,\n",
    "        num_support=num_support,\n",
    "        num_query=num_query,\n",
    "        num_tasks_per_epoch=NUM_TEST_TASKS,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "protonet.test(dataloader_meta_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs330_project_hw2",
   "language": "python",
   "name": "cs330_project_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
